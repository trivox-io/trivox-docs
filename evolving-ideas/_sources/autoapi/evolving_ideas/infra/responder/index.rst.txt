evolving_ideas.infra.responder
==============================

.. py:module:: evolving_ideas.infra.responder

.. autoapi-nested-parse::

   evolving_ideas.infra.responder



Attributes
----------

.. autoapisummary::

   evolving_ideas.infra.responder.logger
   evolving_ideas.infra.responder.LLM_BACKENDS


Classes
-------

.. autoapisummary::

   evolving_ideas.infra.responder.LLMResponder


Functions
---------

.. autoapisummary::

   evolving_ideas.infra.responder.get_llm_backend


Module Contents
---------------

.. py:data:: logger

.. py:data:: LLM_BACKENDS

.. py:function:: get_llm_backend(name: str) -> evolving_ideas.infra.llm_interface.LLMInterface

.. py:class:: LLMResponder

   Wraps the underlying LLM for easier substitution/testing.


   .. py:attribute:: llm
      :type:  evolving_ideas.infra.llm_interface.LLMInterface


   .. py:method:: ask(prompt: str, context='You are a helpful assistant.') -> str

      Ask the LLM a question with a given prompt and context.

      :param prompt: The prompt to send to the LLM.
      :type prompt: str

      :param context: Additional context for the LLM (default is "You are a helpful assistant.").
      :type context: str

      :return: The LLM's response.
      :rtype: str



   .. py:method:: chat(chatlog: list) -> str

      Start a chat session with the LLM.

      :param chatlog: A list of messages to include in the chat context.
      :type chatlog: list

      :return: The LLM's response.
      :rtype: str



